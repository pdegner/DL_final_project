{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/transformers\n",
    "!python transformers/utils/download_glue_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now do it with your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>num_votes</th>\n",
       "      <th>Avg Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a really great route~ with awesome exp...</td>\n",
       "      <td>22</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>from tabvar:     Cool fins to roof~ thin holds...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A safe mixed route with a bit of run out up to...</td>\n",
       "      <td>3</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Start on a slab under a left leaning arched ro...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fun technical climbing. Tricky right off the b...</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words  num_votes  Avg Stars\n",
       "0  This is a really great route~ with awesome exp...         22        2.9\n",
       "1  from tabvar:     Cool fins to roof~ thin holds...          1        2.0\n",
       "2  A safe mixed route with a bit of run out up to...          3        2.7\n",
       "3  Start on a slab under a left leaning arched ro...          1        2.0\n",
       "4  Fun technical climbing. Tricky right off the b...          3        3.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_whole = pd.read_csv(\"data/all_routes_and_desc.csv\")\n",
    "df_whole = df_whole.rename(columns=lambda x: x.strip()) # Removes whitespace around column names\n",
    "df_whole[\"words\"] = df_whole[\"desc\"] + \" \" + df_whole[\"protection\"]\n",
    "df = df_whole[[\"words\", \"num_votes\", \"Avg Stars\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116700 352 116348 True\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with no description\n",
    "bad_df = df[df.words.apply(lambda x: len(str(x))<=5)]\n",
    "new_df = df[~df.words.isin(bad_df.words)]\n",
    "print(len(df), len(bad_df), len(new_df), len(df)-len(bad_df)==len(new_df))\n",
    "df = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-english entries\n",
    "# takes a few minutes...\n",
    "def is_english(x):\n",
    "    try:\n",
    "        return detect(x)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df[\"english\"] = df['words'].apply(lambda x: is_english(x) == 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.english]\n",
    "df = df[[\"words\", \"num_votes\", \"Avg Stars\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>num_votes</th>\n",
       "      <th>Avg Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a really great route~ with awesome exp...</td>\n",
       "      <td>22</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>from tabvar:     Cool fins to roof~ thin holds...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A safe mixed route with a bit of run out up to...</td>\n",
       "      <td>3</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Start on a slab under a left leaning arched ro...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fun technical climbing. Tricky right off the b...</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words  num_votes  Avg Stars\n",
       "0  This is a really great route~ with awesome exp...         22        2.9\n",
       "1  from tabvar:     Cool fins to roof~ thin holds...          1        2.0\n",
       "2  A safe mixed route with a bit of run out up to...          3        2.7\n",
       "3  Start on a slab under a left leaning arched ro...          1        2.0\n",
       "4  Fun technical climbing. Tricky right off the b...          3        3.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(df.num_votes <= 9)[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31022"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now remove rows with less than 10 votes\n",
    "few_votes = np.where(df.num_votes <= 9)[0]\n",
    "for vote in few_votes:\n",
    "    try:\n",
    "        df.drop(vote, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "# df_small = df.drop(few_votes)\n",
    "# df = df_small\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/words_and_stars_no_ninevotes.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, tune DistilBERT with the route data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig, TFAutoModelWithLMHead, TFAutoModel, AutoModel\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/words_and_stars_no_ninevotes.csv')\n",
    "df.replace(4,3.9999999) # prevents errors\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code is a tester for 3000 examples, just to make sure it runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1k = df[:3000]\n",
    "\n",
    "# normalize star values\n",
    "df_1k[\"norm_star\"] = df_1k[\"Avg Stars\"]/4\n",
    "df_1k.replace(1., .9999)\n",
    "\n",
    "df_1k.norm_star.unique()\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "tf_batch = tokenizer(\n",
    "     list(df_1k[\"words\"]),\n",
    "     padding=True,\n",
    "     truncation=True,\n",
    "     return_tensors=\"tf\"\n",
    " )\n",
    "\n",
    "tf_outputs = tf_model(tf_batch, labels = tf.constant(list(df_1k[\"norm_star\"]), dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = [list(df_1k[\"norm_star\"])[i]-float(tf_outputs[0][i]) for i in range(len(df_1k))]\n",
    "star_diff = (sum(loss)/3000)\n",
    "star_diff # off on average by .33 stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_directory = \"models/route_model\"\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code uses all the examples and saves the model\n",
    "\n",
    "Too expensive: I had to do it in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalize star values\n",
    "# df[\"norm_star\"] = df[\"Avg Stars\"]/2\n",
    "# df.head()\n",
    "\n",
    "# # drop null entries\n",
    "# print(len(np.where(pd.isnull(df[\"words\"]))[0])) # 288 null entries\n",
    "# df.dropna(inplace = True)\n",
    "\n",
    "# model_name = \"distilbert-base-uncased\"\n",
    "# tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# tf_batch = tokenizer(\n",
    "#      list(df[\"words\"]),\n",
    "#      padding=True,\n",
    "#      truncation=True,\n",
    "#      return_tensors=\"tf\"\n",
    "#  )\n",
    "\n",
    "# tf_outputs = tf_model(tf_batch, labels = tf.constant(list(df[\"norm_star\"]), dtype=tf.float64))\n",
    "\n",
    "# loss = [list(df[\"norm_star\"])[i]-float(tf_outputs[0][i]) for i in range(len(df))]\n",
    "# star_diff = (sum(loss)/1000)*4\n",
    "# star_diff\n",
    "\n",
    "# # Save the model\n",
    "# save_directory = \"models/route_model\"\n",
    "# tokenizer.save_pretrained(save_directory)\n",
    "# model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, finish tuning by doing 3000 at a time until you have done it all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [(3000,6000), (6000, 9000), (9000, 12000), (12000, 15000), (15000, 18000), (18000, 21000), \n",
    "          (21000, 24000), (24000, 27000), (27000, len(df))]\n",
    "losses = [star_diff]\n",
    "\n",
    "save_directory = \"models/route_model\"\n",
    "\n",
    "model_num = 2\n",
    "tmin = time.time()\n",
    "\n",
    "for chunk in chunks:\n",
    "    t0 = time.time()\n",
    "    \n",
    "    df_1k = df[chunk[0]:chunk[1]]\n",
    "\n",
    "    # normalize star values\n",
    "    df_1k[\"norm_star\"] = df_1k[\"Avg Stars\"]/4\n",
    "    df_1k.replace(1., .9999)\n",
    "\n",
    "    # Reload pretrained model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "    model = TFAutoModel.from_pretrained(save_directory, from_pt=True)\n",
    "\n",
    "    # Initiate tokenizer\n",
    "    tf_batch = tokenizer(list(df_1k[\"words\"]), padding=True, truncation=True, return_tensors=\"tf\")\n",
    "    \n",
    "    # Get outputs\n",
    "    tf_outputs = tf_model(tf_batch, labels = tf.constant(list(df_1k[\"norm_star\"]), dtype=tf.float64))\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = [list(df_1k[\"norm_star\"])[i]-float(tf_outputs[0][i]) for i in range(len(df_1k))]\n",
    "    star_diff = (sum(loss)/(chunk[1]-chunk[0]))\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Save the big model\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "    model.save_pretrained(save_directory)\n",
    "    \n",
    "    # Save the intermediate model\n",
    "    inter_save_directory = \"models/inter_models/model_\" + str(model_num)\n",
    "    tokenizer.save_pretrained(inter_save_directory)\n",
    "    model.save_pretrained(inter_save_directory)\n",
    "    model_num += 1\n",
    "    \n",
    "    \n",
    "    t1 = time.time()\n",
    "    elap = t1-t0\n",
    "    \n",
    "    print(\"%.2f minutes, or %.2f hours have passed for iteration between %s and %s.\" %(elap/60, elap/60/60, chunk[0], chunk[1]))\n",
    "    \n",
    "tmax = time.time()\n",
    "print(\"This code took a total of %.2f minutes, or %.2f hours to run.\" %((tmax-tmin)/60, (tmax-tmin)/60/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [(6000, 9000), (9000, 12000), (12000, 15000), (15000, 18000), (18000, 21000), \n",
    "          (21000, 24000), (24000, 27000), (27000, len(df))]\n",
    "losses = [star_diff]\n",
    "\n",
    "model_num = 3\n",
    "tmin = time.time()\n",
    "\n",
    "for chunk in chunks:\n",
    "    t0 = time.time()\n",
    "    \n",
    "    df_1k = df[chunk[0]:chunk[1]]\n",
    "\n",
    "    # normalize star values\n",
    "    df_1k[\"norm_star\"] = df_1k[\"Avg Stars\"]/4\n",
    "    df_1k.replace(1., .9999)\n",
    "\n",
    "    # Reload pretrained model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "    model = TFAutoModel.from_pretrained(save_directory, from_pt=False)\n",
    "\n",
    "    # Initiate tokenizer\n",
    "    tf_batch = tokenizer(list(df_1k[\"words\"]), padding=True, truncation=True, return_tensors=\"tf\")\n",
    "    \n",
    "    # Get outputs\n",
    "    tf_outputs = tf_model(tf_batch, labels = tf.constant(list(df_1k[\"norm_star\"]), dtype=tf.float64))\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = [list(df_1k[\"norm_star\"])[i]-float(tf_outputs[0][i]) for i in range(len(df_1k))]\n",
    "    star_diff = (sum(loss)/(chunk[1]-chunk[0]))\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Save the big model\n",
    "    save_directory = \"models/route_model\"\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "    model.save_pretrained(save_directory)\n",
    "    \n",
    "    # Save the intermediate model\n",
    "    inter_save_directory = \"models/inter_models/model_\" + str(model_num)\n",
    "    tokenizer.save_pretrained(inter_save_directory)\n",
    "    model.save_pretrained(inter_save_directory)\n",
    "    model_num += 1\n",
    "    \n",
    "    t1 = time.time()\n",
    "    elap = t1-t0\n",
    "    \n",
    "    print(\"%.2f minutes, or %.2f hours have passed for iteration between %s and %s.\" %(elap/60, elap/60/60, chunk[0], chunk[1]))\n",
    "    \n",
    "tmax = time.time()\n",
    "print(\"This code took a total of %.2f minutes, or %.2f hours to run.\" %((tmax-tmin)/60, (tmax-tmin)/60/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, use this model as the base for the 1,099 gear reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gear_df = pd.read_csv(\"data/trailspace_gear_reviews.csv\")\n",
    "gear_df = gear_df.rename(columns=lambda x: x.strip())\n",
    "gear_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize rating\n",
    "gear_df[\"norm_rating\"] = gear_df[\"rating\"]/5*2\n",
    "gear_df[\"norm_rating\"] = gear_df[\"norm_rating\"].replace(2., 1.99999)\n",
    "gear_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = TFAutoModel.from_pretrained(save_directory, from_pt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some more training\n",
    "tf_batch = tokenizer(\n",
    "     list(gear_df[\"rating_text\"]),\n",
    "     padding=True,\n",
    "     truncation=True,\n",
    "     return_tensors=\"tf\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_outputs = tf_model(tf_batch, labels = tf.constant(list(gear_df[\"norm_rating\"]), dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = [list(gear_df[\"norm_rating\"])[i]-float(tf_outputs[0][i]) for i in range(len(gear_df))]\n",
    "star_diff = (sum(loss)/len(gear_df))*5\n",
    "star_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_directory = \"models/trailspace_and_route_model\"\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, tune a DistillBERT model only on the gear reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'activation_13', 'vocab_projector', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_503']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "gear_df = pd.read_csv(\"data/trailspace_gear_reviews.csv\")\n",
    "gear_df = gear_df.rename(columns=lambda x: x.strip())\n",
    "\n",
    "# normalize rating\n",
    "gear_df[\"norm_rating\"] = gear_df[\"rating\"]/5*2\n",
    "gear_df[\"norm_rating\"] = gear_df[\"norm_rating\"].replace(2., 1.99999)\n",
    "gear_df.head()\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "tf_batch = tokenizer(\n",
    "     list(gear_df[\"rating_text\"]),\n",
    "     padding=True,\n",
    "     truncation=True,\n",
    "     return_tensors=\"tf\"\n",
    " )\n",
    "\n",
    "tf_outputs = tf_model(tf_batch, labels = tf.constant(list(gear_df[\"norm_rating\"]), dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_directory = \"models/trailspace_model\"\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
